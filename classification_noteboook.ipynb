{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch, wandb, time\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,random_split,SubsetRandomSampler, ConcatDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Internal Imports\n",
    "from model import AudioClassifier\n",
    "from util import WavDataSet, SoundDS, PlotSpectrogram, predict, classes, classes_reverse\n",
    "from training_standard import training, inference  \n",
    "from training_k_fold import train_epoch, valid_epoch\n",
    "\n",
    "# GPU Setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "\n",
    "# # Create index\n",
    "# data = WavDataSet(\"D:/Documents/Samples/Beat Packs\")\n",
    "# print(data)\n",
    "\n",
    "# # Save CSV\n",
    "# data.csv(\"./data/edm_no_loops.csv\")\n",
    "\n",
    "# # Check Results \n",
    "# df = pd.read_csv(\"./data/edm_no_loops.csv\") \n",
    "# print(df.head())\n",
    "# print(f\"Dataset Length: {len(df)}\")\n",
    "\n",
    "# Create Pandas Dataframe from CSV\n",
    "df = pd.read_csv(\"./data/edm_no_loops.csv\")\n",
    "df = df[['path', 'class']]\n",
    "\n",
    "# Prepare Batches of Data using the Dataloader\n",
    "# Random 80:20 Split of Train:Validate\n",
    "myds = SoundDS(df)\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.67)\n",
    "num_val = num_items-num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0381da46e8e4b66a31440502acd5701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n",
      "torch.Size([64, 2, 64, 344])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\development\\generative-beatpack\\classification_noteboook.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/development/generative-beatpack/classification_noteboook.ipynb#ch0000002?line=10'>11</a>\u001b[0m train_dl \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_ds, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/development/generative-beatpack/classification_noteboook.ipynb#ch0000002?line=11'>12</a>\u001b[0m val_dl \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(val_ds, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/development/generative-beatpack/classification_noteboook.ipynb#ch0000002?line=13'>14</a>\u001b[0m training(model, train_dl, device\u001b[39m=\u001b[39;49mdevice, num_epochs\u001b[39m=\u001b[39;49mnum_epochs, lr\u001b[39m=\u001b[39;49mlr, max_lr\u001b[39m=\u001b[39;49mmax_lr, logger\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\development\\generative-beatpack\\training_standard.py:28\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, train_dl, device, num_epochs, lr, max_lr, logger)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/training_standard.py?line=24'>25</a>\u001b[0m total_prediction \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/training_standard.py?line=26'>27</a>\u001b[0m \u001b[39m# Repeat for each batch in the training set\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/development/generative-beatpack/training_standard.py?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl):\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/training_standard.py?line=28'>29</a>\u001b[0m     \u001b[39m# Get inputs and labels on device\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/training_standard.py?line=29'>30</a>\u001b[0m     inputs \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device) \n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/training_standard.py?line=30'>31</a>\u001b[0m     labels \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:578\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=574'>575</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=575'>576</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=576'>577</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=577'>578</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=578'>579</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=579'>580</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=580'>581</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=581'>582</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:618\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=615'>616</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=616'>617</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=617'>618</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=618'>619</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=619'>620</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataset.py?line=287'>288</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataset.py?line=288'>289</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataset.py?line=289'>290</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32md:\\development\\generative-beatpack\\util.py:120\u001b[0m, in \u001b[0;36mSoundDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/development/generative-beatpack/util.py?line=117'>118</a>\u001b[0m         rechan \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mrechannel(reaud, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel) \n\u001b[0;32m    <a href='file:///d%3A/development/generative-beatpack/util.py?line=118'>119</a>\u001b[0m         dur_aud \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mpad_trunc(rechan, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration) \u001b[39m# sig, sr\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/development/generative-beatpack/util.py?line=119'>120</a>\u001b[0m         sgram \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39;49mspectro_gram(dur_aud, n_mels\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, n_fft\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, hop_len\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///d%3A/development/generative-beatpack/util.py?line=120'>121</a>\u001b[0m         aug_sgram \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mspectro_augment(sgram, max_mask_pct\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, n_freq_masks\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_time_masks\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/development/generative-beatpack/util.py?line=122'>123</a>\u001b[0m \t\t\t\t\u001b[39m# plot_spectrogram(sgram[0], title=f\"Random Sample Spec: {os.path.basename(audio_file)}\")\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/development/generative-beatpack/util.py?line=123'>124</a>\u001b[0m \t\t\t\t\u001b[39m# plot_spectrogram(aug_sgram[0], title=f\"Random Augmented Spec: {os.path.basename(audio_file)}\")\u001b[39;00m\n",
      "File \u001b[1;32md:\\development\\generative-beatpack\\audio_util.py:87\u001b[0m, in \u001b[0;36mAudioUtil.spectro_gram\u001b[1;34m(aud, n_mels, n_fft, hop_len)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/audio_util.py?line=83'>84</a>\u001b[0m top_db \u001b[39m=\u001b[39m \u001b[39m120\u001b[39m\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/audio_util.py?line=85'>86</a>\u001b[0m \u001b[39m# Spec has shape [channel, n_mels, time] where channel is mono, stereo, etc\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/development/generative-beatpack/audio_util.py?line=86'>87</a>\u001b[0m spec \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39;49mMelSpectrogram(sr, n_fft\u001b[39m=\u001b[39;49mn_fft, hop_length\u001b[39m=\u001b[39;49mhop_len, n_mels\u001b[39m=\u001b[39;49mn_mels)(sig)\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/audio_util.py?line=88'>89</a>\u001b[0m \u001b[39m# Convert to dB\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/development/generative-beatpack/audio_util.py?line=89'>90</a>\u001b[0m spec \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mAmplitudeToDB(top_db\u001b[39m=\u001b[39mtop_db)(spec)\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:612\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=603'>604</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=604'>605</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=605'>606</a>\u001b[0m \u001b[39m    waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=608'>609</a>\u001b[0m \u001b[39m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=609'>610</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=610'>611</a>\u001b[0m specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspectrogram(waveform)\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=611'>612</a>\u001b[0m mel_specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmel_scale(specgram)\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=612'>613</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:388\u001b[0m, in \u001b[0;36mMelScale.forward\u001b[1;34m(self, specgram)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=378'>379</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=379'>380</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=380'>381</a>\u001b[0m \u001b[39m    specgram (Tensor): A spectrogram STFT of dimension (..., freq, time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=383'>384</a>\u001b[0m \u001b[39m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=384'>385</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=386'>387</a>\u001b[0m \u001b[39m# (..., time, freq) dot (freq, n_mels) -> (..., n_mels, time)\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=387'>388</a>\u001b[0m mel_specgram \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(specgram\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfb)\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=389'>390</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mel_specgram\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Standard Training, Validation and Prediction\n",
    "batch_size = 64\n",
    "num_epochs = 1000\n",
    "lr = 0.002\n",
    "max_lr = 0.05\n",
    "\n",
    "model = AudioClassifier()\n",
    "model = torch.load('models/standard/model_lr0.0002_mlr0.01_e1000_b64.pt')\n",
    "model.to(device)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "training(model, train_dl, device=device, num_epochs=num_epochs, lr=lr, max_lr=max_lr, logger=None)\n",
    "# inference(model, val_dl, device=device, logger=None)\n",
    "\n",
    "# torch.save(model, f'models/model_lr{lr}_mlr{max_lr}_e{num_epochs}_b{batch_size}.pt')\n",
    "\n",
    "# Make a Prediction\n",
    "# predict(model, \"examples/TR-808Kick01.wav\", \"kick\")\n",
    "# for path, classID in df.values:\n",
    "\t\t# predict(model, path, classes_reverse[classID], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K Fold Cross Validation\n",
    "batch_size = 64 \n",
    "num_epochs = 10\n",
    "lr = 0.002\n",
    "dataset = ConcatDataset([train_ds, val_ds])\n",
    "\n",
    "# For Training, use a High Epoch and Low K\n",
    "# For Evaluation, use a Low Epoch and K of ~10\n",
    "k = 10\n",
    "torch.manual_seed(42)\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf = {}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "    wandb.init(project=\"beatpack-ai\", entity=\"parabyl\", job_type='k-fold', name=f'test1e_f{fold+1}') # Running this line creates a new 'run'\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler) \n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    # model = AudioClassifier()\n",
    "    model = torch.load('models/model_lr0.0002_mlr0.01_e1000_b64.pt')\n",
    "    model.to(device)\n",
    "    wandb.watch(model, log_freq=100)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    \n",
    "    # Load Previous Model\n",
    "    # cp = torch.load('models/zesty-salad-125_e512_f3_b128.pt')\n",
    "    # model.load_state_dict(cp['model_state_dict'])\n",
    "    # optimizer.load_state_dict(cp['optimizer_state_dict'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, steps_per_epoch=int(len(train_loader)), epochs=num_epochs, anneal_strategy='linear')\n",
    "    \n",
    "    print(f\"Fold {fold+1}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss, train_correct = train_epoch(model, device, train_loader, criterion, optimizer, scheduler)\n",
    "        test_loss, test_correct = valid_epoch(model, device, test_loader, criterion)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs} AVG Training Loss: {train_loss:.3f} AVG Test Loss: {test_loss:.3f} AVG Training Acc: {train_acc:.2f}% AVG Test Acc: {test_acc:.2f}% Time: {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        wandb.log({\n",
    "            'Epoch': epoch+1, \n",
    "            'Avg. Training Loss': train_loss, \n",
    "            'Avg. Test Loss': test_loss, \n",
    "            'Avg. Training Accuracy': train_acc, \n",
    "            'Avg. Test Accuracy': test_acc})\n",
    "\n",
    "               \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "    foldperf[f'fold{fold}'] = history\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, f'models/{wandb.run.name}_e{num_epochs}_f{fold}_b{batch_size}.pt')\n",
    "    wandb.join()\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Test Report\n",
    "\n",
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "for f in range(0,k):\n",
    "    tl_f.append(np.mean(foldperf[f'fold{f}']['train_loss']))\n",
    "    testl_f.append(np.mean(foldperf[f'fold{f}']['test_loss']))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf[f'fold{f}']['train_acc']))\n",
    "    testa_f.append(np.mean(foldperf[f'fold{f}']['test_acc']))\n",
    "\n",
    "print(f'K={k} fold cross validation complete')\n",
    "print(f\"Avg Training Loss: {np.mean(tl_f):.2f} Avg Test Loss: {np.mean(testl_f):.2f} Avg Training Acc: {np.mean(ta_f)} Avg Test Acc: {np.mean(testa_f)}\")     "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c7a21014fc0903b333c528e26b532495acabffc408f92f7990944da68b6f70a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

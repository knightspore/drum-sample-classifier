{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f04a934c66442281af0383ad2112e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Development\\generative-beatpack\\generation_notebook.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Development/generative-beatpack/generation_notebook.ipynb#ch0000000?line=70'>71</a>\u001b[0m \u001b[39mwith\u001b[39;00m trange(num_epochs, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Development/generative-beatpack/generation_notebook.ipynb#ch0000000?line=71'>72</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tepoch:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Development/generative-beatpack/generation_notebook.ipynb#ch0000000?line=72'>73</a>\u001b[0m         \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Development/generative-beatpack/generation_notebook.ipynb#ch0000000?line=73'>74</a>\u001b[0m             idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Development/generative-beatpack/generation_notebook.ipynb#ch0000000?line=75'>76</a>\u001b[0m             \u001b[39m# zero the gradients on each iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:578\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=574'>575</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=575'>576</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=576'>577</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=577'>578</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=578'>579</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=579'>580</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=580'>581</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=581'>582</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:618\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=615'>616</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=616'>617</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=617'>618</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=618'>619</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/dataloader.py?line=619'>620</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Development\\generative-beatpack\\util.py:119\u001b[0m, in \u001b[0;36mSoundDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/util.py?line=116'>117</a>\u001b[0m         dur_aud \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mpad_trunc(rechan, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration) \u001b[39m# sig, sr\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/util.py?line=117'>118</a>\u001b[0m         sgram \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mspectro_gram(dur_aud, n_mels\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, n_fft\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, hop_len\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> <a href='file:///d%3A/Development/generative-beatpack/util.py?line=118'>119</a>\u001b[0m         aug_sgram \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39;49mspectro_augment(sgram, max_mask_pct\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, n_freq_masks\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, n_time_masks\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/util.py?line=120'>121</a>\u001b[0m \t\t\t\t\u001b[39m# plot_spectrogram(sgram[0], title=f\"Random Sample Spec: {os.path.basename(audio_file)}\")\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/util.py?line=121'>122</a>\u001b[0m \t\t\t\t\u001b[39m# plot_spectrogram(aug_sgram[0], title=f\"Random Augmented Spec: {os.path.basename(audio_file)}\")\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/util.py?line=123'>124</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m aug_sgram, class_id, idx\n",
      "File \u001b[1;32md:\\Development\\generative-beatpack\\audio_util.py:104\u001b[0m, in \u001b[0;36mAudioUtil.spectro_augment\u001b[1;34m(spec, max_mask_pct, n_freq_masks, n_time_masks)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/audio_util.py?line=101'>102</a>\u001b[0m freq_mask_param \u001b[39m=\u001b[39m max_mask_pct \u001b[39m*\u001b[39m n_mels\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/audio_util.py?line=102'>103</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_freq_masks):\n\u001b[1;32m--> <a href='file:///d%3A/Development/generative-beatpack/audio_util.py?line=103'>104</a>\u001b[0m     aug_spec \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39;49mFrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/audio_util.py?line=105'>106</a>\u001b[0m time_mask_param \u001b[39m=\u001b[39m max_mask_pct \u001b[39m*\u001b[39m n_steps\n\u001b[0;32m    <a href='file:///d%3A/Development/generative-beatpack/audio_util.py?line=106'>107</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_time_masks):\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:1171\u001b[0m, in \u001b[0;36m_AxisMasking.forward\u001b[1;34m(self, specgram, mask_value)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=1168'>1169</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmask_along_axis_iid(specgram, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_param, mask_value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp)\n\u001b[0;32m   <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=1169'>1170</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/transforms/_transforms.py?line=1170'>1171</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmask_along_axis(specgram, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask_param, mask_value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis, p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp)\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\lib\\site-packages\\torchaudio\\functional\\functional.py:873\u001b[0m, in \u001b[0;36mmask_along_axis\u001b[1;34m(specgram, mask_param, mask_value, axis, p)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/functional/functional.py?line=869'>870</a>\u001b[0m value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m mask_param\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/functional/functional.py?line=870'>871</a>\u001b[0m min_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (specgram\u001b[39m.\u001b[39msize(axis) \u001b[39m-\u001b[39m value)\n\u001b[1;32m--> <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/functional/functional.py?line=872'>873</a>\u001b[0m mask_start \u001b[39m=\u001b[39m (min_value\u001b[39m.\u001b[39;49mlong())\u001b[39m.\u001b[39;49msqueeze()\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/functional/functional.py?line=873'>874</a>\u001b[0m mask_end \u001b[39m=\u001b[39m (min_value\u001b[39m.\u001b[39mlong() \u001b[39m+\u001b[39m value\u001b[39m.\u001b[39mlong())\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m    <a href='file:///c%3A/Users/c/anaconda3/lib/site-packages/torchaudio/functional/functional.py?line=874'>875</a>\u001b[0m mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, specgram\u001b[39m.\u001b[39mshape[axis], device\u001b[39m=\u001b[39mspecgram\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39mspecgram\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generator V2 Notes\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "from model import AudioClassifier\n",
    "from util import SoundDS, PlotSpectrogram\n",
    "\n",
    "# Based on https://towardsdatascience.com/build-a-super-simple-gan-in-pytorch-54ba349920e4\n",
    "\n",
    "# We need a generator that takes in noise\n",
    "# and generates torch.Size([2, 64, 344])\n",
    "# 2 = num_channels (this is `inputs` which the model is run on)\n",
    "# 64 = Mel freq_bands\n",
    "# 344 = time_steps\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 44032)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, 2, 64, 344)\n",
    "        return nn.Tanh()(x)\n",
    "    \n",
    "\"\"\"\n",
    "Network training procedure\n",
    "Every step both the loss for disciminator and generator is updated\n",
    "Discriminator aims to classify reals and fakes\n",
    "Generator aims to generate images as realistic as possible\n",
    "\"\"\"\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "D_lr, G_lr = 0.002, 0.001\n",
    "\n",
    "D = AudioClassifier()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=D_lr)\n",
    "# cp = torch.load('D:/Development/generative-beatpack/models/zesty-salad-125_e512_f3_b128.pt')\n",
    "# D.load_state_dict(cp['model_state_dict'])\n",
    "# D_optimizer.load_state_dict(cp['optimizer_state_dict'])\n",
    "D.to(device)\n",
    "\n",
    "G = generator()\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=G_lr)\n",
    "# gcp = torch.load('./Generator_epoch_19.pth') # e 410\n",
    "# G.load_state_dict(cp, strict=False)\n",
    "G.to(device)\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "gloss = nn.BCELoss()\n",
    "\n",
    "# Load Data\n",
    "\n",
    "df = pd.read_csv('data/edm_no_loops.csv')\n",
    "df = df[['path', 'class']]\n",
    "myds = SoundDS(df)\n",
    "train_dl = torch.utils.data.DataLoader(myds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "with trange(num_epochs, unit=\"epoch\") as tepoch:\n",
    "    for epoch in tepoch:\n",
    "        for idx, data in enumerate(train_dl):\n",
    "            idx += 1\n",
    "            \n",
    "            # zero the gradients on each iteration\n",
    "            G_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate examples of even real data\n",
    "            true_data = data[0].to(device)\n",
    "            inputs_m, inputs_s = true_data.mean(), true_data.std()\n",
    "            true_data = (true_data - inputs_m) / inputs_s\n",
    "            true_labels = torch.ones(true_data.shape[0], 10).to(device)\n",
    "            \n",
    "            # Create noise\n",
    "            noise = (torch.rand(true_data.shape[0], 128) - 0.5) / 0.5\n",
    "            noise = noise.to(device)\n",
    "            generated_data = G(noise)\n",
    "            fake_label = torch.zeros(generated_data.shape[0], 10).to(device)\n",
    "\n",
    "            \n",
    "            # Train the Generator...\n",
    "            #\n",
    "            # We flip the labels here\n",
    "            # and don't train discriminator.\n",
    "            # because we want the generator\n",
    "            # to make things the discriminator \n",
    "            # classifies as true.\n",
    "            \n",
    "            generator_discriminator_out = D(generated_data)\n",
    "            generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "            generator_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            \n",
    "            # Train the discriminator on true/generated data\n",
    "            D_optimizer.zero_grad()\n",
    "            true_discriminator_out = D(true_data)\n",
    "            true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "            \n",
    "            # Add .detach() here thing about this\n",
    "            generator_discriminator_out = D(generated_data.detach())\n",
    "            generator_discriminator_loss = loss(generator_discriminator_out, fake_label)\n",
    "            discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "            discriminator_loss.backward()\n",
    "            D_optimizer.step()\n",
    "            \n",
    "            if idx == len(train_dl):\n",
    "                print(\"Random Sample from Batch\")\n",
    "                real_sg = true_data.view(-1, 2, 64, 344)[0].cpu().detach().numpy()\n",
    "                PlotSpectrogram(real_sg[0], title=\"Real Sample: \"+str(discriminator_loss.item())+\" loss\")\n",
    "            \n",
    "            if idx == len(train_dl):\n",
    "                gen_sg = generated_data.view(-1, 2, 64, 344)[0].cpu().detach().numpy()\n",
    "                PlotSpectrogram(gen_sg[0], title=f\"Generated Sample: \"+str(generator_loss.item())+\"\")\n",
    "\n",
    "            tepoch.set_postfix(b=f\"{idx}/{len(train_dl)}\", e=f\"{epoch}/{num_epochs}\", D_loss=discriminator_loss.item(), G_loss=generator_loss.item())\n",
    "            sleep(0.1)\n",
    "            \n",
    "    torch.save(G, 'firstTest_e{}.pth'.format(epoch))\n",
    "    print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator V1 Notes\n",
    "\n",
    "# Based on https://towardsdatascience.com/building-a-gan-with-pytorch-237b4b07ca9a\n",
    "\n",
    "# We need a generator that takes in noise\n",
    "# and generates torch.Size([2, 64, 344])\n",
    "# 2 = num_channels (this is `inputs` which the model is run on)\n",
    "# 64 = Mel freq_bands\n",
    "# 344 = time_steps\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 44032)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, 2, 64, 344)\n",
    "        return nn.Tanh()(x)\n",
    "    \n",
    "\"\"\"\n",
    "Network training procedure\n",
    "Every step both the loss for disciminator and generator is updated\n",
    "Discriminator aims to classify reals and fakes\n",
    "Generator aims to generate images as realistic as possible\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "from time import sleep\n",
    "\n",
    "D = AudioClassifier()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=0.002)\n",
    "# cp = torch.load('D:/Development/generative-beatpack/models/zesty-salad-125_e512_f3_b128.pt')\n",
    "# D.load_state_dict(cp['model_state_dict'])\n",
    "# D_optimizer.load_state_dict(cp['optimizer_state_dict'])\n",
    "D.to(device)\n",
    "\n",
    "G = generator()\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=0.001)\n",
    "# gcp = torch.load('./Generator_epoch_19.pth') # e 410\n",
    "# G.load_state_dict(cp, strict=False)\n",
    "G.to(device)\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "gloss = nn.BCELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "with trange(num_epochs, unit=\"epoch\") as tepoch:\n",
    "    for epoch in tepoch:\n",
    "        for idx, data in enumerate(train_dl):\n",
    "            idx += 1\n",
    "\n",
    "            # Train discriminator (classifier)\n",
    "            # Fake inputs are from generator\n",
    "            # Real inputs are classified as 1 and fake as 0\n",
    "            # Real inputs are from train_dl\n",
    "            real_inputs = data[0].to(device)\n",
    "            inputs_m, inputs_s = real_inputs.mean(), real_inputs.std()\n",
    "            real_inputs = (real_inputs - inputs_m) / inputs_s\n",
    "            real_outputs = D(real_inputs)\n",
    "            real_label = torch.ones(real_inputs.shape[0], 10).to(device)\n",
    "            \n",
    "            noise = (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "            noise = noise.to(device)\n",
    "            fake_inputs = G(noise)\n",
    "            fake_outputs = D(fake_inputs)\n",
    "            fake_label = torch.zeros(fake_inputs.shape[0], 10).to(device)\n",
    "            \n",
    "            outputs = torch.cat((real_outputs, fake_outputs), 0)\n",
    "            targets = torch.cat((real_label, fake_label), 0)\n",
    "            \n",
    "            D_loss = loss(outputs,targets)\n",
    "            D_optimizer.zero_grad()\n",
    "            D_loss.backward()\n",
    "            D_optimizer.step()\n",
    "            \n",
    "            if idx == len(train_dl):\n",
    "                print(\"Random Sample from Batch\")\n",
    "                real_sg = real_inputs.view(-1, 2, 64, 344)[0].cpu().detach().numpy()\n",
    "                PlotSpectrogram(real_sg[0], title=\"Real Sample: \"+str(D_loss.item())+\" loss\")\n",
    "                \n",
    "\n",
    "            \n",
    "            # Training the generator\n",
    "            # For generator, we want it to believe everything is 1\n",
    "            noise = (torch.rand(real_inputs.shape[0], 128)-0.5)/0.5\n",
    "            noise = noise.to(device)\n",
    "            \n",
    "            fake_inputs = G(noise)\n",
    "            fake_outputs = D(fake_inputs)\n",
    "            fake_targets = torch.ones([fake_inputs.shape[0], 10]).to(device)\n",
    "            G_loss = gloss(fake_outputs, fake_targets)\n",
    "            G_optimizer.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            \n",
    "            if idx == len(train_dl):\n",
    "                gen_sg = fake_inputs.view(-1, 2, 64, 344)[0].cpu().detach().numpy()\n",
    "                PlotSpectrogram(gen_sg[0], title=f\"Generated Noise Sample: \"+str(G_loss.item())+\"\")\n",
    "\n",
    "            tepoch.set_postfix(b=f\"{idx}/{len(train_dl)}\", e=f\"{epoch}/{num_epochs}\", D_loss=D_loss.item(), G_loss=G_loss.item())\n",
    "            sleep(0.1)\n",
    "            \n",
    "    torch.save(G, 'firstTest_e{}.pth'.format(epoch))\n",
    "    print('Model saved.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c7a21014fc0903b333c528e26b532495acabffc408f92f7990944da68b6f70a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
